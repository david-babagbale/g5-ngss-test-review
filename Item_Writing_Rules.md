# Comprehensive Rules for Writing High-Quality Grade 5 Science Test Questions

> **Purpose:** This document synthesizes research-based item-writing guidelines from Haladyna, Downing & Rodriguez (2002), NGSS three-dimensional assessment frameworks, state assessment design specifications, and analysis of exemplary items from top-performing state tests. It is specifically tailored for creating a Grade 5 NGSS-aligned standardized Science test.

---

## PART 1: FOUNDATIONAL ITEM-WRITING RULES

### A. Content & Cognitive Design (Rules 1–8)

**Rule 1: Every item must target a specific Performance Expectation (PE) and cognitive behavior.**
- Map each item to exactly one NGSS PE (e.g., 5-PS1-1, 5-LS1-1).
- Identify the intended Depth of Knowledge (DOK 1–3) before writing.
- Use a test blueprint (two-way grid) to ensure coverage across PEs and DOK levels.

**Rule 2: Assess important, standards-aligned content only.**
- Never test trivial facts, obscure vocabulary, or "gotcha" knowledge.
- Every question should connect to a Disciplinary Core Idea (DCI) that 5th graders have been taught.
- If you can't explain why a student needs to know this, don't test it.

**Rule 3: Use novel scenarios and phenomena to assess higher-order thinking.**
- NEVER use textbook-verbatim language or identical examples from instruction.
- Present a new phenomenon, data set, or scenario that requires students to APPLY their understanding.
- Paraphrase or recontextualize content so students must transfer knowledge, not simply recall.
- Example: Instead of "What are the states of matter?", present data about an unfamiliar substance and ask students to classify its state based on observable properties.

**Rule 4: Keep items independent of each other.**
- The answer to one question must never depend on getting another question correct.
- Exception: Item clusters (like NY's approach) where questions share a stimulus but each can be answered independently.
- Never provide clues in one item that give away the answer to another.

**Rule 5: Avoid overly specific or overly general content.**
- Don't ask about a niche fact that only appears in one curriculum (e.g., specific page numbers, exact lab procedures).
- Don't ask questions so broad they become meaningless (e.g., "Why is science important?").
- Target the "sweet spot" of the PE's scope.

**Rule 6: Ensure content is age-appropriate and within Grade 5 experience.**
- Never assume knowledge typically taught in higher grades.
- Never assume specific life experiences (e.g., visiting an ocean, using specific lab equipment).
- Vocabulary must be within a 5th grader's reading level (Flesch-Kincaid Grade 5–6).
- **CRITICAL:** Do not assume students know unit conversions (e.g., 1 mL water = 1 g) unless explicitly stated in the question. If a question requires this knowledge, provide it as given information.

**Rule 7: Avoid trick questions that mislead students.**
- A student who understands the content should be able to answer correctly.
- Items should differentiate between students who understand vs. don't understand the science—NOT between careful vs. careless readers.
- Never use double negatives, misleading context, or deliberate ambiguity.

**Rule 8: Write items that are three-dimensional (NGSS-specific).**
- Each item should integrate at least two of the three NGSS dimensions:
  - **DCI** (Disciplinary Core Idea): The science content
  - **SEP** (Science and Engineering Practice): What students DO (analyze data, argue from evidence, develop models, etc.)
  - **CCC** (Crosscutting Concept): The connecting theme (patterns, cause & effect, systems, energy & matter, etc.)
- High-quality items naturally require students to USE a practice WITH a core idea, not just recall facts.

---

### B. Stem Construction (Rules 9–15)

**Rule 9: State the stem as a clear, direct question.**
- Question format ("Which statement best explains...?") is superior to completion format ("The reason for X is ___").
- Research consistently shows question-form stems are clearer and more accessible.

**Rule 10: Never leave a blank in the beginning or middle of the stem.**
- Bad: "___ is the process by which plants make food."
- Good: "What is the process by which plants make food?"

**Rule 11: Make directions in the stem crystal clear.**
- Students should know exactly what is being asked after reading the stem once.
- Include specific cues: "Based on the data in the table...", "Using information from the model...", "Which statement BEST explains..."

**Rule 12: Eliminate window dressing (unnecessary verbiage).**
- Every word in the stem should serve a purpose.
- Remove decorative language, irrelevant backstory, or filler.
- "Verbosity is an enemy of clarity" (Haladyna, 2004).
- BAD: "Maria is a curious student who loves science. One day in class, she was doing an experiment about plants. Her teacher asked her to observe what happened when a plant was placed in sunlight. What did Maria most likely observe?"
- GOOD: "A student placed a plant in sunlight for several hours. What would the student most likely observe?"

**Rule 13: Word the stem positively; avoid negative phrasing.**
- Avoid: "Which of the following is NOT true about ecosystems?"
- If negative phrasing is absolutely necessary, highlight it: "Which is NOT an example..."
- Negative stems add cognitive load without testing science knowledge.

**Rule 14: Include the central idea in the stem, not the options.**
- The stem should contain enough information that a knowledgeable student could answer BEFORE seeing the options.
- Students should be able to formulate a mental answer before looking at choices.

**Rule 15: Provide sufficient context for stimulus-based items.**
- When using data tables, graphs, models, or diagrams, ensure all necessary information is present.
- Label all axes, include units, provide keys/legends.
- State explicitly what the student should use: "Based on the data in Table 1..."

---

### C. Answer Choice Design — THE MOST CRITICAL SECTION (Rules 16–28)

**Rule 16: ⚠️ KEEP ALL OPTIONS APPROXIMATELY THE SAME LENGTH AND COMPLEXITY. ⚠️**
- This is the #1 most common flaw in poorly written items.
- The correct answer must NOT be noticeably longer, more detailed, or more "scientific-sounding" than distractors.
- **Length test:** If the correct answer is more than ~20% longer than the average distractor, rewrite it.
- **Complexity test:** If the correct answer uses more technical language or more qualifiers than distractors, rewrite it.
- **HOW TO FIX:** Either (a) shorten the correct answer, or (b) lengthen the distractors to match.
- Example of VIOLATION:
  - A) The object speeds up
  - B) The object slows down
  - C) The unbalanced force causes the object to accelerate in the direction of the net force, changing its speed and/or direction of motion ← OBVIOUSLY CORRECT
  - D) The object stays the same
- Example of CORRECT approach:
  - A) The object speeds up in the direction of the applied force
  - B) The object slows down and eventually stops moving
  - C) The object accelerates in the direction of the net force
  - D) The object continues at the same speed and direction

**Rule 17: Use four answer options (A, B, C, D).**
- Research supports 3–5 options; four is the standard for state assessments.
- More than four options makes it harder to write plausible distractors without padding.
- Fewer than three doesn't provide enough discrimination.

**Rule 18: Place options in logical or numerical order.**
- Numerical answers: ascending or descending order.
- Qualitative answers: alphabetical or conceptual grouping.
- Random arrangement makes items harder to read for no psychometric benefit.

**Rule 19: Keep all options mutually exclusive (no overlap).**
- No option should be a subset of another.
- BAD: A) "plants", B) "trees", C) "animals", D) "insects" — trees are plants, insects are animals.

**Rule 20: Keep all options homogeneous in content and grammatical structure.**
- All options should be the same part of speech, same tense, same format.
- If one option is a full sentence, ALL should be full sentences.
- If one option is a single phrase, ALL should be single phrases.
- Options should belong to the same conceptual category.

**Rule 21: NEVER use "All of the above" or "None of the above."**
- These are universally condemned by assessment experts.
- "All of the above" can be eliminated if a student knows just one option is wrong.
- "None of the above" tests test-taking skill, not science knowledge.

**Rule 22: Avoid absolute terms in distractors.**
- Words like "always," "never," "all," "none," "only," "completely" are red flags for test-wise students.
- These are almost always wrong, so savvy students eliminate them automatically.
- Use qualified language in ALL options equally: "most likely," "usually," "often."

**Rule 23: Avoid clang associations and grammatical cues.**
- A "clang association" is when a word in the stem matches a word in the correct answer.
- BAD stem: "What type of ENERGY conversion..." → correct answer contains the word "ENERGY."
- Grammatical cue: stem says "An ___" → only one option starts with a vowel.
- Check: Could a student with ZERO science knowledge eliminate options based on grammar or word matching alone?

**Rule 24: Make distractors plausible and based on real student misconceptions.**
- Each distractor should represent a genuine, common error in student thinking.
- Sources for good distractors: Published misconception research, student work analysis, common wrong answers from field testing.
- An implausible distractor wastes an option slot and effectively reduces the item to fewer choices.

**Rule 25: Incorporate common student errors into distractors.**
- If students commonly confuse "weathering" with "erosion," make that a distractor.
- If students think plants get food from soil (not photosynthesis), use that misconception.
- This makes items discriminate between students who truly understand and those who hold misconceptions.

**Rule 26: Use true-but-irrelevant statements as distractors.**
- A great distractor is a TRUE statement that doesn't answer the specific question asked.
- This tests whether students can distinguish between "true" and "relevant."
- Example for "Why do objects fall to the ground?": "Objects have mass" is TRUE but doesn't explain WHY they fall.

**Rule 27: Distribute the correct answer position evenly.**
- Across the full test, correct answers should appear roughly equally in positions A, B, C, D.
- Target: Each position holds ~25% of correct answers (±5%).
- Never place the correct answer in the same position for more than 3 consecutive items.

**Rule 28: Ensure there is one and ONLY one defensibly correct answer.**
- Have multiple subject-matter experts review each item.
- If an expert can argue for a different answer, revise the item.
- The correct answer must be unambiguously right; distractors must be unambiguously wrong for the right reasons.

---

### D. Formatting & Accessibility (Rules 29–33)

**Rule 29: Use grade-appropriate vocabulary and sentence structure.**
- Reading level should not exceed Grade 5–6.
- Define or contextualize any necessary scientific terms.
- Avoid jargon, idioms, or culturally specific references.

**Rule 30: Minimize examinee reading time.**
- Shorter items are better items (if clarity is maintained).
- Science content should be the challenge, NOT reading comprehension.
- For ELL students and students with reading difficulties, conciseness is an equity issue.

**Rule 31: Use consistent grammar, punctuation, and spelling.**
- All options should start with the same capitalization convention.
- All options should end with the same punctuation (or none).
- Parallel grammatical structure across all options.

**Rule 32: Format items vertically (options stacked, not side by side).**
- Each option on its own line for clarity.
- Exception: Very short options (single words) may be arranged horizontally with adequate spacing.

**Rule 33: Ensure visual accessibility.**
- All images must have clear labels and sufficient contrast.
- Tables and graphs must have headers, units, and legends.
- Font size must be readable (minimum 12pt for print).
- Color should never be the sole means of conveying information.

---

## PART 2: NGSS-SPECIFIC ASSESSMENT DESIGN PRINCIPLES

### Three-Dimensional Item Design

**Principle 1: Items must be phenomenon-based.**
- Root every item in an observable phenomenon or problem.
- The phenomenon should be grade-appropriate, engaging, and novel (not used in instruction).
- Students should use science knowledge to EXPLAIN the phenomenon, not just recall facts about it.

**Principle 2: Integrate Science and Engineering Practices (SEPs).**
- Items should require students to DO science, not just KNOW science.
- Key SEPs for Grade 5 assessment:
  - **Developing and Using Models** — Interpret, evaluate, or complete a model
  - **Planning and Carrying Out Investigations** — Identify variables, evaluate procedures
  - **Analyzing and Interpreting Data** — Read graphs/tables, identify patterns, draw conclusions
  - **Engaging in Argument from Evidence** — Evaluate claims, identify supporting evidence
  - **Constructing Explanations** — Explain WHY a phenomenon occurs using science concepts
  - **Obtaining, Evaluating, and Communicating Information** — Interpret scientific text/media

**Principle 3: Embed Crosscutting Concepts (CCCs).**
- Items should naturally incorporate one or more CCCs:
  - **Patterns** — Identify and explain patterns in data or nature
  - **Cause and Effect** — Explain mechanisms; predict outcomes
  - **Scale, Proportion, and Quantity** — Reason about relative sizes and rates
  - **Systems and System Models** — Analyze components and interactions
  - **Energy and Matter** — Track flows, cycles, and conservation
  - **Structure and Function** — Relate form to purpose
  - **Stability and Change** — Explain equilibrium and disruption

**Principle 4: Use item clusters (stimulus-based sets) for deeper assessment.**
- Present a rich scenario (data table, model, investigation description) followed by 3–5 questions.
- Each question in the cluster targets a different dimension combination.
- This approach mirrors how NY, NJ, and CA design their state assessments.
- Cluster approach reduces reading load per item while enabling deeper assessment.

**Principle 5: Assess knowledge-in-use, not knowledge-in-isolation.**
- Students should apply knowledge to a new context, not simply retrieve memorized facts.
- If a question can be answered by memorizing a definition, it needs revision.
- Acceptable: "Based on the data, which variable most likely caused the change in plant growth?"
- Unacceptable: "What is a variable?"

---

## PART 3: DIFFICULTY & DISCRIMINATION DESIGN

### P-Value Targets for a Balanced Test

| Difficulty Level | P-Value Range | Target % of Items | Purpose |
|---|---|---|---|
| Easy | 0.70–1.00 | ~20% (8 items) | Build confidence, identify very low performers |
| Medium | 0.40–0.70 | ~60% (24 items) | Core discrimination; separate proficient from non-proficient |
| Hard | 0.15–0.40 | ~20% (8 items) | Identify advanced students; stretch items |

### DOK Distribution

| DOK Level | Description | Target % | Examples |
|---|---|---|---|
| DOK 1 | Recall & Reproduction | 15–20% | Identify, define, recognize, match |
| DOK 2 | Skill/Concept Application | 55–65% | Compare, classify, interpret data, explain |
| DOK 3 | Strategic Thinking | 20–25% | Analyze, evaluate, justify, design, construct argument |

---

## PART 4: CRITICAL FLAWS TO AVOID (Based on Audit of Common Errors)

### Flaw 1: Correct Answer Conspicuously Longer Than Distractors
- **Why it happens:** Writers naturally add qualifiers and precision to make the correct answer "more correct."
- **Why it's harmful:** Test-wise students pick the longest answer without knowing the science.
- **Fix protocol:** After writing, compare word counts. If the correct answer is >20% longer than the average distractor, revise ALL options to similar length.

### Flaw 2: Correct Answer Uses More Technical/Scientific Language
- **Why it happens:** The correct answer naturally uses precise scientific terminology.
- **Why it's harmful:** Students learn to pick the "most scientific-sounding" option.
- **Fix protocol:** Either simplify the correct answer's language OR add comparable scientific language to distractors.

### Flaw 3: Assuming Grade-Inappropriate Prior Knowledge
- **Why it happens:** Item writers are subject-matter experts who forget what 10-year-olds know.
- **Why it's harmful:** Tests content knowledge that wasn't taught, creating construct-irrelevant variance.
- **Fix protocol:** For every fact required to answer the question, ask: "Was this explicitly taught in the G5 curriculum?" If not, it must be provided in the stimulus.
- **Specific examples:**
  - Don't assume students know density relationships (1 mL water = 1 g)
  - Don't assume students know specific chemical formulas
  - Don't assume students can do multi-step unit conversions
  - Don't assume students know names of specific minerals, biomes, or organisms not in the curriculum

### Flaw 4: Only One Scientifically Plausible Distractor
- **Why it happens:** Writers create one good distractor and fill the rest with obviously wrong answers.
- **Why it's harmful:** Reduces the item to a binary choice; inflates P-values artificially.
- **Fix protocol:** Each distractor should represent a genuine misconception. If you can't write 3 plausible distractors, consider restructuring the question.

### Flaw 5: Grammatical or Logical Cues
- **Why it happens:** Oversight during writing.
- **Detection checklist:**
  - Does the stem's grammar (a/an, singular/plural) match only the correct answer?
  - Is there a word in the stem that appears only in the correct answer option?
  - Are absolute terms (always, never) only in distractors?
  - Is the correct answer the only option with a hedge word (usually, most likely)?

### Flaw 6: Testing Reading Comprehension Instead of Science
- **Why it happens:** Complex stimulus material with dense text.
- **Why it's harmful:** Disadvantages ELLs, students with reading difficulties, and measures the wrong construct.
- **Fix protocol:** Can a strong science student with average reading skills answer this? If the reading demand is the primary challenge, simplify the text or add visual supports.

---

## PART 5: THE ITEM REVIEW CHECKLIST

Use this checklist for EVERY item before it enters the test:

### Content Validity
- [ ] Maps to a specific NGSS PE
- [ ] Assesses important, grade-level content
- [ ] Uses a novel phenomenon (not from textbook/instruction)
- [ ] Integrates at least 2 NGSS dimensions (DCI + SEP or DCI + CCC)
- [ ] Has one and only one defensibly correct answer
- [ ] Content is within Grade 5 curriculum scope
- [ ] No assumed knowledge beyond what is given or grade-appropriate

### Stem Quality
- [ ] Clear question or prompt
- [ ] No unnecessary verbiage
- [ ] Positively worded (no "NOT" or "EXCEPT" unless highlighted)
- [ ] Central idea is in the stem, not the options
- [ ] Stimulus material is complete, labeled, and grade-appropriate

### Option Quality
- [ ] Four options (A–D)
- [ ] All options approximately the same length (within 20%)
- [ ] All options parallel in grammatical structure
- [ ] All options homogeneous in content category
- [ ] No "All of the above" or "None of the above"
- [ ] No absolute terms (always, never) used unevenly
- [ ] No grammatical or clang association cues
- [ ] Each distractor represents a genuine misconception or common error
- [ ] Correct answer position varies across the test (~25% per position)

### Accessibility & Fairness
- [ ] Reading level ≤ Grade 6
- [ ] No cultural, socioeconomic, or gender bias
- [ ] Images are clear, labeled, and necessary
- [ ] Alt-text or description available for all images
- [ ] Vocabulary is grade-appropriate or defined in context

### Difficulty & Discrimination
- [ ] Target DOK level identified and appropriate
- [ ] Estimated difficulty (easy/medium/hard) assigned
- [ ] Item discriminates between students who understand and those who don't

---

## PART 6: STATE TEST RANKINGS BY NGSS ALIGNMENT

The following ranking indicates how closely each state's science assessment aligns with NGSS, from most aligned to least. Tests higher on this list should be given greater weight when studying question design for an NGSS-aligned test.

| Rank | State Test | Standards Base | NGSS Alignment | Notes |
|---|---|---|---|---|
| 1 | **NY Elementary Science (NYSSLS)** | New York State P-12 Science Learning Standards | ★★★★★ Direct adoption | NY adopted NGSS virtually verbatim. Test items map directly to NGSS PEs. Item map uses NGSS PE codes. Three-dimensional cluster-based design. |
| 2 | **CA CAST** | California NGSS (CA NGSS) | ★★★★★ Direct adoption with CA additions | California adopted NGSS with minor state-specific additions. CAST uses performance tasks + discrete items. Three-dimensional by design. Computer-adaptive. |
| 3 | **NJ NJSLA-Science** | NJ Student Learning Standards–Science | ★★★★★ Direct adoption | New Jersey adopted NGSS directly. Three-dimensional assessment design. Uses stimulus-based tasks (SBTs). |
| 4 | **SC READY Science** | SC College-and-Career-Ready Science Standards 2021 | ★★★★☆ Framework-based | SC's 2021 standards are closely aligned with the NRC Framework and NGSS. Items explicitly tag SEP, DCI, and CCC. High-quality three-dimensional design. |
| 5 | **FL Statewide Science Assessment** | Next Generation Sunshine State Standards (NGSSS) | ★★★☆☆ Moderate alignment | Florida's NGSSS shares some DNA with the NRC Framework but was developed independently. Less emphasis on SEPs and CCCs. More traditional MC format. |
| 6 | **TX STAAR Science** | Texas Essential Knowledge and Skills (TEKS) | ★★☆☆☆ Low alignment | Texas did not adopt NGSS. TEKS organizes content differently, places topics at different grade levels, and has different emphasis. Some content overlap but structural misalignment. |

### Implications for Test Design
- **Primary models to emulate:** NY, CA, NJ (Tier 1 — direct NGSS adoption)
- **Secondary models:** SC (Tier 2 — strong framework alignment, excellent item metadata)
- **Reference only:** FL, TX (Tier 3 — useful for studying item formats but not NGSS alignment)

---

## PART 7: SPECIFIC QUALITY PATTERNS FROM EXEMPLARY STATE TESTS

### What NY Does Well (Grade 5 Science, 2024)
1. **Cluster-based design:** Questions 1–4 all relate to "Animal Senses" stimulus, reducing reading load while enabling multi-dimensional assessment.
2. **Rich stimuli:** Data tables, models, maps, and graphs are integral to questions, not decorative.
3. **Mix of MC and constructed response:** Approximately 50/50 split enables assessing both recognition and production.
4. **Answer option balance:** MC options are consistently matched in length and complexity.
5. **P-value data published:** Enables analysis of actual difficulty (ranging from 0.07 to 0.66).
6. **Direct PE mapping:** Every item maps to a specific NGSS PE with PE code published.

### What SC Does Well (Grade 4 Science, 2024)
1. **Full three-dimensional metadata:** Every item explicitly identifies SEP, DCI, and CCC.
2. **Diverse item types:** Selected response, evidence-based selected response (EBSR), multi-select, drag-and-drop, and drop-down formats.
3. **DOK and difficulty estimates published:** Allows transparent assessment design.
4. **EBSR format (two-part items):** Part A asks for a claim/conclusion; Part B asks for supporting evidence. This naturally integrates the "Argument from Evidence" SEP.
5. **Phenomenon-based scenarios:** Wind turbines at the beach, Grand Canyon rock layers, plastic owls deterring birds—all real-world, engaging phenomena.

### What FL Does Adequately
1. **Clean, straightforward MC items** that are easy to read.
2. **Good stimulus passages** (like the radiometer data).
3. **Consistent 4-option format.**
4. **Limitation:** Items tend toward DOK 1–2 and single-dimensional (content recall) rather than three-dimensional.

---

## APPENDIX: QUICK REFERENCE — THE 10 GOLDEN RULES

For everyday reference during item writing, these are the 10 most critical rules:

1. **One PE, one item.** Map every question to a specific Performance Expectation.
2. **Novel phenomenon.** Root every question in a new, real-world scenario.
3. **Equal-length options.** Correct answer and distractors must be within 20% of each other's length.
4. **Plausible distractors.** Every wrong answer should represent a real student misconception.
5. **No assumed knowledge.** If a fact is needed and isn't grade-level common knowledge, provide it.
6. **Clear, concise stem.** Students should understand what's being asked in one reading.
7. **Three-dimensional.** Integrate DCI + SEP + CCC wherever possible.
8. **Grade-appropriate vocabulary.** Reading level ≤ Grade 6; define technical terms in context.
9. **No cues.** No length cues, grammatical cues, clang associations, or absolute terms.
10. **Expert review.** Every item reviewed by at least two people before use.

---

*Document compiled from: Haladyna, Downing & Rodriguez (2002), Haladyna & Downing (1989), NGSS Assessment Resources (Achieve/NSTA), NY 2024 Elementary Science Released Items, SC READY Grade 4 Science Sample Items (2024), FL Grade 5 Science Sample Test Materials (2024), CAST Assessment Design specifications, and best practices from the National Research Council Framework for K-12 Science Education.*
